{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "received-rebel",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Machine learning</h2>\n",
    "<h3>Data clustering algorithms implementation</h3>\n",
    "\n",
    "This notebook presents the implementation and application of some traditional machine learning clustering algorithms.\n",
    "In the following cell I make the necessary imports, including tensorflow, which provides us with the image set on which we experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "entire-anchor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "from math import sqrt, floor, log\n",
    "import cmath\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-october",
   "metadata": {},
   "source": [
    "Importing the dataset. I won't use the labels, since we're practicing unsupervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complete-pittsburgh",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-venice",
   "metadata": {},
   "source": [
    "Below I store the names of each clothing category in a list, so as to present the score values for each class seperately when the time to print stats and results comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respective-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-greeting",
   "metadata": {},
   "source": [
    "Normalising the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "going-pilot",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data normalisation by dividing with 255, which is the maximum pixel value and corresponds to deep black\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-dietary",
   "metadata": {},
   "source": [
    "I extract a small subset of the initial data (1000 images), because the algorithms would take very long to finish for the initial whole set of data. I make sure the subset has evenly distributed data between each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operational-street",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_subset_size = 1000\n",
    "train_subset_images = []\n",
    "train_subset_labels = []\n",
    "\n",
    "for category_index in range(10):\n",
    "    remaining_from_category = train_subset_size / 10 # we need equally distributed data per category\n",
    "    image_index = 0\n",
    "    \n",
    "    while remaining_from_category > 0 :\n",
    "        if train_labels[image_index] == category_index:\n",
    "            train_subset_images.append(train_images[image_index])\n",
    "            train_subset_labels.append(train_labels[image_index])\n",
    "            remaining_from_category -= 1\n",
    "        image_index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "color-wealth",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# R1 dataset : 28x28 = 784 feature vector\n",
    "\n",
    "train_subset_images = np.array(train_subset_images)\n",
    "train_subset_labels = np.array(train_subset_labels)\n",
    "\n",
    "nsamples, nx, ny = train_subset_images.shape \n",
    "train_subset_images_R1 = train_subset_images.reshape((nsamples,nx*ny))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worldwide-poetry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9]\n"
     ]
    }
   ],
   "source": [
    "print(train_subset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-stewart",
   "metadata": {},
   "source": [
    "Declaring a euclidean distance calculation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "according-economics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(v1, v2):\n",
    "    return np.sqrt(np.sum((v1-v2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-capitol",
   "metadata": {},
   "source": [
    "Trying out euclidean_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adopted-virgin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.196373794859472\n"
     ]
    }
   ],
   "source": [
    "print(euclidean_distance(np.array([-7, -4]), np.array([17, 6.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-shock",
   "metadata": {},
   "source": [
    "Bellow I implement the k_means algorithm from scratch. I create a model with a predict() function which if parameterised by the vector data, it will cluster them without the need for training (unsupervised learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fancy-drunk",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_iter is the maximum iterations that we allow in case we don't converge earlier\n",
    "# k = 10 for k clusters (since the clothing categories are actually 10)\n",
    "\n",
    "class k_means:\n",
    "    def __init__(self, K = 10, max_iter = 300, tolerance = 0.001):\n",
    "        self.K = K\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        # list of sample indices for each cluster\n",
    "        self.clusters = [[] for i in range(self.K)]\n",
    "        \n",
    "        # mean feature vector for each cluster\n",
    "        self.centroids = []\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        print(X.shape)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        print(\"Starting algorithm\")\n",
    "        # initialise centroids (10 of the feature vectors will be picked randomly)\n",
    "        random_sample_indices = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.centroids = [self.X[index] for index in random_sample_indices]\n",
    "        \n",
    "        # optimisation\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # update clusters\n",
    "            self.clusters = self._create_clusters(self.centroids)\n",
    "            \n",
    "            # update centroids\n",
    "            centroids_old = self.centroids # in order to check for convergence\n",
    "            self.centroids = self._get_centroids(self.clusters)\n",
    "            \n",
    "            # check for convergence\n",
    "            if self._is_converged(centroids_old, self.centroids):\n",
    "                break\n",
    "                \n",
    "            print(\"Finished iteration\", i)\n",
    "            \n",
    "        # return cluster labels\n",
    "        return self.__get_cluster_labels(self.clusters)\n",
    "    \n",
    "    def __get_cluster_labels(self, clusters):\n",
    "        labels = np.empty(self.n_samples)\n",
    "        for cluster_index, cluster in enumerate(clusters):\n",
    "            for sample_index in cluster:\n",
    "                labels[sample_index] = cluster_index\n",
    "                \n",
    "        return labels\n",
    "       \n",
    "    # assign each sample to the cluster of the closest centroid\n",
    "    def _create_clusters(self, centroids):\n",
    "        clusters = [[] for i in range(self.K)]\n",
    "        \n",
    "        for index, sample in enumerate(self.X):\n",
    "            centroid_index = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_index].append(index)\n",
    "            \n",
    "        return clusters\n",
    "    \n",
    "    # calculate the distances of the current sample from each centroid (according to L2 distance)\n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        distances = [euclidean_distance(sample, centroid) for centroid in centroids]\n",
    "        closest_index = np.argmin(distances)\n",
    "        return closest_index\n",
    "    \n",
    "    # calculate the centroids as the mean value of the samples in each cluster\n",
    "    def _get_centroids(self, clusters):\n",
    "        centroids = np.zeros((self.K, self.n_features))\n",
    "        \n",
    "        for cluster_index, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.X[cluster])\n",
    "            centroids[cluster_index] = cluster_mean\n",
    "            \n",
    "        return centroids\n",
    "   \n",
    "    # check for convergence\n",
    "    def _is_converged(self, centroids_old, centroids):\n",
    "        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n",
    "        return sum(distances) <= self.tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-macintosh",
   "metadata": {},
   "source": [
    "Testing the model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chemical-ability",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "Starting algorithm\n",
      "Finished iteration 0\n",
      "Finished iteration 1\n",
      "Finished iteration 2\n",
      "Finished iteration 3\n",
      "Finished iteration 4\n",
      "Finished iteration 5\n",
      "Finished iteration 6\n",
      "Finished iteration 7\n",
      "Finished iteration 8\n",
      "Finished iteration 9\n",
      "Finished iteration 10\n",
      "Finished iteration 11\n",
      "Finished iteration 12\n",
      "Finished iteration 13\n",
      "Finished iteration 14\n",
      "Finished iteration 15\n",
      "Finished iteration 16\n",
      "Finished iteration 17\n",
      "Finished iteration 18\n",
      "Finished iteration 19\n",
      "Finished iteration 20\n",
      "Finished iteration 21\n",
      "Finished iteration 22\n",
      "Finished iteration 23\n",
      "Finished iteration 24\n",
      "Finished iteration 25\n",
      "Finished iteration 26\n",
      "Finished iteration 27\n",
      "Finished iteration 28\n",
      "Finished iteration 29\n",
      "Finished iteration 30\n",
      "Finished iteration 31\n",
      "Finished iteration 32\n",
      "Finished iteration 33\n",
      "Finished iteration 34\n",
      "Finished iteration 35\n",
      "Finished iteration 36\n",
      "Finished iteration 37\n",
      "Finished iteration 38\n",
      "Finished iteration 39\n",
      "Finished iteration 40\n",
      "Finished iteration 41\n",
      "Finished iteration 42\n",
      "Finished iteration 43\n",
      "Finished iteration 44\n",
      "Finished iteration 45\n",
      "Finished iteration 46\n",
      "Finished iteration 47\n",
      "Finished iteration 48\n",
      "Finished iteration 49\n",
      "Finished iteration 50\n",
      "Finished iteration 51\n",
      "Finished iteration 52\n",
      "Finished iteration 53\n",
      "Finished iteration 54\n",
      "Finished iteration 55\n",
      "Finished iteration 56\n",
      "Finished iteration 57\n",
      "Finished iteration 58\n",
      "Finished iteration 59\n",
      "Finished iteration 60\n",
      "Finished iteration 61\n",
      "Finished iteration 62\n",
      "Finished iteration 63\n",
      "Finished iteration 64\n",
      "Finished iteration 65\n",
      "Finished iteration 66\n",
      "Finished iteration 67\n",
      "Finished iteration 68\n",
      "Finished iteration 69\n"
     ]
    }
   ],
   "source": [
    "k_means_model = k_means(K = 10, max_iter = 300)\n",
    "k_means_predictions = k_means_model.predict(train_subset_images_R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "developing-sentence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 7 5 3 6 6 1 3 5 3 0 5 5 6 0 5 0 0 0 5 7 5 9 6 7 9 5 3 0 4 4 6 5 6 3 9 0\n",
      " 6 3 8 8 3 5 0 9 7 3 5 9 5 8 0 5 7 4 9 3 0 0 3 5 8 9 5 3 5 0 0 5 0 3 8 0 8\n",
      " 5 4 0 6 0 0 5 5 5 0 0 6 0 3 6 8 7 0 7 9 0 6 7 0 3 3 8 8 8 9 8 8 9 5 8 8 5\n",
      " 8 9 8 7 8 7 5 9 3 9 8 8 5 9 8 9 6 9 8 8 9 8 7 9 8 8 3 9 8 8 7 9 8 8 9 8 9\n",
      " 7 8 9 9 9 8 9 8 8 7 8 8 8 9 4 5 8 8 8 9 8 5 9 9 9 8 9 9 8 8 8 8 7 9 9 8 5\n",
      " 8 9 9 8 1 8 8 8 6 8 3 8 9 8 9 0 2 4 5 6 2 9 4 9 8 4 3 0 0 0 5 2 6 2 6 6 4\n",
      " 0 5 5 4 0 4 0 4 8 0 9 0 9 9 9 5 4 4 1 5 9 5 3 8 4 5 5 0 6 6 0 2 5 0 4 4 2\n",
      " 0 6 0 0 0 6 6 4 0 6 6 9 2 8 4 4 1 0 8 6 0 4 6 6 4 4 9 0 6 4 9 9 3 4 4 2 4\n",
      " 9 0 4 0 8 3 3 9 3 6 5 5 5 9 6 3 7 9 5 9 3 6 8 9 8 7 5 7 9 9 5 6 7 4 5 9 7\n",
      " 8 5 9 8 5 3 5 3 5 5 8 8 8 1 6 8 0 0 8 3 9 5 3 6 5 4 8 8 9 6 7 5 3 3 5 5 8\n",
      " 3 5 7 8 8 7 9 9 9 5 8 3 5 3 3 9 0 3 6 8 6 7 8 3 3 8 7 7 5 3 7 5 0 5 0 3 6\n",
      " 5 7 6 4 8 4 0 0 6 0 3 6 0 5 6 4 0 6 4 4 4 7 2 2 5 3 1 3 0 6 8 7 0 4 5 0 6\n",
      " 8 0 4 7 3 0 6 4 4 4 6 4 3 5 4 6 2 4 4 0 4 4 9 0 6 5 8 9 3 0 3 0 6 3 8 6 6\n",
      " 4 8 4 6 0 6 5 3 0 0 0 6 6 3 6 6 4 6 3 1 9 7 7 1 8 7 8 1 1 7 9 1 7 1 1 9 7\n",
      " 9 7 7 8 7 9 5 1 9 6 7 8 7 1 1 1 7 9 1 9 9 1 9 1 7 7 7 7 9 7 7 7 7 7 9 1 7\n",
      " 1 1 1 7 1 7 7 1 9 7 1 1 7 1 9 1 1 7 1 1 1 9 1 7 5 1 1 1 7 1 9 9 1 7 7 9 9\n",
      " 1 7 7 1 1 1 9 7 0 6 1 4 0 6 5 8 0 5 5 3 9 9 8 3 4 8 0 9 3 6 3 0 3 5 6 5 9\n",
      " 7 9 7 0 4 4 6 0 0 3 8 8 5 9 6 6 9 6 8 9 3 9 6 4 4 5 4 3 9 6 0 8 5 0 6 3 7\n",
      " 9 0 5 3 6 7 6 2 5 5 8 5 1 3 9 9 0 8 6 8 3 0 4 5 5 0 0 8 0 4 0 9 0 8 9 1 9\n",
      " 9 9 9 9 7 8 9 9 7 7 7 9 1 9 9 9 8 9 1 8 7 8 7 8 7 7 7 9 9 9 7 8 9 9 7 7 7\n",
      " 7 9 7 9 9 9 7 9 1 7 7 9 8 7 9 8 7 7 8 8 9 8 8 9 9 9 7 7 5 7 7 9 7 1 7 7 1\n",
      " 7 9 1 7 8 8 9 7 1 9 9 7 7 5 7 9 7 8 7 7 9 7 8 6 8 8 6 6 6 0 6 5 0 8 8 0 5\n",
      " 3 3 9 5 5 4 4 3 9 3 4 6 2 9 5 5 8 5 8 3 6 3 5 5 8 5 9 0 4 6 6 0 0 9 3 3 7\n",
      " 6 6 3 5 2 7 5 0 3 0 9 9 0 0 0 7 6 0 4 0 0 8 3 0 3 3 4 3 8 8 3 3 3 5 7 4 3\n",
      " 5 8 6 8 7 5 6 9 3 6 6 0 6 6 3 3 4 3 0 0 8 9 5 5 9 3 3 8 6 8 9 5 8 7 5 6 5\n",
      " 8 0 5 8 3 8 3 3 5 8 5 4 8 5 1 3 8 5 8 8 5 5 8 8 6 8 6 9 3 8 5 5 3 6 6 8 9\n",
      " 8 3 3 6 3 3 8 3 8 3 5 5 6 5 5 6 8 9 5 5 6 9 3 5 9 6 9 8 9 5 5 3 5 3 8 3 3\n",
      " 5]\n"
     ]
    }
   ],
   "source": [
    "# convert to integer array because the items will be used for indexing\n",
    "\n",
    "k_means_predictions = k_means_predictions.astype(int)\n",
    "print(k_means_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-granny",
   "metadata": {},
   "source": [
    "Purity calculation function implementation. This function calculates purity for each seperate cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "potential-economics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_purity(predictions, number_of_classes, train_labels):\n",
    "    \n",
    "    cluster_labels = [[] for i in range(number_of_classes)]\n",
    "    for i in range(len(predictions)):\n",
    "        cluster_labels[predictions[i]].append(train_labels[i]) # get the actual categories for each cluster\n",
    "      \n",
    "    clusters_to_labels = []\n",
    "    \n",
    "    for i in range(len(cluster_labels)):\n",
    "        most_occured = max(set(cluster_labels[i]), key=cluster_labels[i].count)\n",
    "        if most_occured not in clusters_to_labels:\n",
    "            clusters_to_labels.append(most_occured)\n",
    "        else:\n",
    "            while most_occured in clusters_to_labels:\n",
    "                cluster_labels[i] = [x for x in cluster_labels[i] if x != most_occured]\n",
    "                most_occured = max(set(cluster_labels[i]), key=cluster_labels[i].count)\n",
    "            clusters_to_labels.append(most_occured)\n",
    "                \n",
    "    print(clusters_to_labels)\n",
    "        \n",
    "    purity_per_class = np.zeros((10,), dtype=float)\n",
    "    items_per_class = np.zeros((10,), dtype=int)\n",
    "    for i in range(len(predictions)):\n",
    "        if clusters_to_labels[predictions[i]] == train_labels[i]:\n",
    "            purity_per_class[train_labels[i]] += 1\n",
    "        items_per_class[train_labels[i]] += 1\n",
    "        \n",
    "    for i in range(len(purity_per_class)):\n",
    "        purity_per_class[i] = purity_per_class[i] / items_per_class[i]\n",
    "        \n",
    "    return purity_per_class\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-valley",
   "metadata": {},
   "source": [
    "Implementing an f-measure calculation function. It calculates the f-measure for each cluster seperately and then print out the overall f-measure as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subtle-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_F_measure(predictions, number_of_classes, train_labels):\n",
    "    \n",
    "    cluster_labels = [[] for i in range(number_of_classes)]\n",
    "    for i in range(len(predictions)):\n",
    "        cluster_labels[predictions[i]].append(train_labels[i]) # get the actual categories for each cluster\n",
    "      \n",
    "    clusters_to_labels = []\n",
    "    for i in range(len(cluster_labels)):\n",
    "        most_occured = max(set(cluster_labels[i]), key=cluster_labels[i].count)\n",
    "        if most_occured not in clusters_to_labels:\n",
    "            clusters_to_labels.append(most_occured)\n",
    "        else:\n",
    "            while most_occured in clusters_to_labels:\n",
    "                cluster_labels[i] = [x for x in cluster_labels[i] if x != most_occured]\n",
    "                most_occured = max(set(cluster_labels[i]), key=cluster_labels[i].count)\n",
    "            clusters_to_labels.append(most_occured)\n",
    "                \n",
    "    print(clusters_to_labels)\n",
    "    \n",
    "    TP_per_cluster = np.zeros((10,), dtype=int)\n",
    "    FP_per_cluster = np.zeros((10,), dtype=int)\n",
    "    FN_per_cluster = np.zeros((10,), dtype=int)\n",
    "    \n",
    "    for cluster_index, cluster in enumerate(cluster_labels):\n",
    "        for label in cluster:\n",
    "            if label == clusters_to_labels[cluster_index]:\n",
    "                TP_per_cluster[cluster_index] += 1\n",
    "            else:\n",
    "                FN_per_cluster[cluster_index] += 1\n",
    "                false_predicted_cluster_index = clusters_to_labels.index(label)\n",
    "                FP_per_cluster[false_predicted_cluster_index] += 1\n",
    "                \n",
    "    f_measure_per_cluster = np.zeros((10,), dtype=float)\n",
    "    precision_measure_per_cluster = np.zeros((10,), dtype=float)\n",
    "    recall_measure_per_cluster = np.zeros((10,), dtype=float)\n",
    "    \n",
    "    for i in range(10):\n",
    "        precision_measure_per_cluster[i] = TP_per_cluster[i] / (TP_per_cluster[i] + FP_per_cluster[i])\n",
    "        recall_measure_per_cluster[i] = TP_per_cluster[i] / (TP_per_cluster[i] + FN_per_cluster[i])\n",
    "        f_measure_per_cluster[i] = 2*((precision_measure_per_cluster[i] * recall_measure_per_cluster[i]) / (precision_measure_per_cluster[i] + recall_measure_per_cluster[i]))\n",
    "        print(\"Cluster\", i ,\" f-measure:\", f_measure_per_cluster[i])\n",
    "        \n",
    "    total_f_measure = np.sum(f_measure_per_cluster)\n",
    "    print(\"Method total f-measure:\", total_f_measure)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-clerk",
   "metadata": {},
   "source": [
    "Printing out k-means results using the functions I implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "downtown-realtor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 2, 9, 4, 3, 8, 7, 1, 6]\n",
      "K_means with R1 purity \n",
      "\n",
      "T-shirt/top purity: 0.25\n",
      "Trouser purity: 0.48\n",
      "Pullover purity: 0.08\n",
      "Dress purity: 0.21\n",
      "Coat purity: 0.2\n",
      "Sandal purity: 0.38\n",
      "Shirt purity: 0.14\n",
      "Sneaker purity: 0.38\n",
      "Bag purity: 0.16\n",
      "Ankle boot purity: 0.22\n"
     ]
    }
   ],
   "source": [
    "k_means_purity = calculate_purity(k_means_predictions, 10, train_subset_labels)\n",
    "print(\"K_means with R1 purity \\n\")\n",
    "for i in range(len(k_means_purity)):\n",
    "    print(class_names[i] + \" purity: \" + str(k_means_purity[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-landscape",
   "metadata": {},
   "source": [
    "Printing out overall purity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "atlantic-affairs",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25000000000000006\n"
     ]
    }
   ],
   "source": [
    "purity = np.mean(k_means_purity)\n",
    "print(purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-chassis",
   "metadata": {},
   "source": [
    "Printing f-measure results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "guided-atlantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_means with R1 f-measure \n",
      "\n",
      "[0, 5, 2, 9, 4, 3, 8, 7, 1, 6]\n",
      "Cluster 0  f-measure: 0.26881720430107525\n",
      "Cluster 1  f-measure: 0.5629629629629629\n",
      "Cluster 2  f-measure: 0.17582417582417584\n",
      "Cluster 3  f-measure: 0.24444444444444444\n",
      "Cluster 4  f-measure: 0.3252032520325203\n",
      "Cluster 5  f-measure: 0.25454545454545463\n",
      "Cluster 6  f-measure: 0.17679558011049723\n",
      "Cluster 7  f-measure: 0.42696629213483145\n",
      "Cluster 8  f-measure: 0.43438914027149317\n",
      "Cluster 9  f-measure: 0.18181818181818182\n",
      "Method total f-measure: 3.0517666884456367\n"
     ]
    }
   ],
   "source": [
    "print(\"K_means with R1 f-measure \\n\")\n",
    "calculate_F_measure(k_means_predictions, 10, train_subset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-nicaragua",
   "metadata": {},
   "source": [
    "Creating a function which creates a histogram for each input image, based on the bins we set as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "marine-eight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def histogram(images, bins):\n",
    "    hist = np.zeros((len(images), bins))\n",
    "    \n",
    "    for image_index, image in enumerate(images):\n",
    "        for row in range(len(image)):\n",
    "            for column in range(len(image[0])):\n",
    "                bin_index = floor(image[row][column] / (1/bins))\n",
    "                if bin_index == bins:\n",
    "                    bin_index = bins - 1\n",
    "                hist[image_index][bin_index] += 1\n",
    "                \n",
    "    return hist          \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-bathroom",
   "metadata": {},
   "source": [
    "The histogram dataset will be named R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "failing-groove",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R2 = histogram(train_subset_images, bins=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-bikini",
   "metadata": {},
   "source": [
    "Printing out one histogram from the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vietnamese-captain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[327.   4.   4.   0.   2.  10.  10.   3.   6.   9.   4.   5.   5.   3.\n",
      "   7.   7.   5.   5.   1.   2.   5.   3.   1.   7.  38. 136.  87.  32.\n",
      "  12.   9.  11.  24.]\n"
     ]
    }
   ],
   "source": [
    "print(R2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-transmission",
   "metadata": {},
   "source": [
    "Testing the k-means algorithm on the R2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "composed-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32)\n",
      "Starting algorithm\n",
      "Finished iteration 0\n",
      "Finished iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\j\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\users\\j\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration 2\n",
      "Finished iteration 3\n",
      "Finished iteration 4\n",
      "Finished iteration 5\n",
      "Finished iteration 6\n",
      "Finished iteration 7\n",
      "Finished iteration 8\n",
      "Finished iteration 9\n",
      "Finished iteration 10\n",
      "Finished iteration 11\n",
      "Finished iteration 12\n",
      "Finished iteration 13\n",
      "Finished iteration 14\n",
      "Finished iteration 15\n",
      "Finished iteration 16\n",
      "Finished iteration 17\n",
      "Finished iteration 18\n",
      "Finished iteration 19\n",
      "Finished iteration 20\n",
      "Finished iteration 21\n",
      "Finished iteration 22\n",
      "Finished iteration 23\n",
      "Finished iteration 24\n",
      "Finished iteration 25\n",
      "Finished iteration 26\n",
      "Finished iteration 27\n",
      "Finished iteration 28\n",
      "Finished iteration 29\n",
      "Finished iteration 30\n",
      "Finished iteration 31\n",
      "Finished iteration 32\n",
      "Finished iteration 33\n",
      "Finished iteration 34\n",
      "Finished iteration 35\n",
      "Finished iteration 36\n",
      "Finished iteration 37\n",
      "Finished iteration 38\n",
      "Finished iteration 39\n",
      "Finished iteration 40\n",
      "Finished iteration 41\n",
      "Finished iteration 42\n",
      "Finished iteration 43\n",
      "Finished iteration 44\n",
      "Finished iteration 45\n",
      "Finished iteration 46\n",
      "Finished iteration 47\n",
      "Finished iteration 48\n",
      "Finished iteration 49\n",
      "Finished iteration 50\n",
      "Finished iteration 51\n",
      "Finished iteration 52\n",
      "Finished iteration 53\n",
      "Finished iteration 54\n",
      "Finished iteration 55\n",
      "Finished iteration 56\n",
      "Finished iteration 57\n",
      "Finished iteration 58\n",
      "Finished iteration 59\n",
      "Finished iteration 60\n",
      "Finished iteration 61\n",
      "Finished iteration 62\n",
      "Finished iteration 63\n",
      "Finished iteration 64\n",
      "Finished iteration 65\n",
      "Finished iteration 66\n",
      "Finished iteration 67\n",
      "Finished iteration 68\n",
      "Finished iteration 69\n",
      "Finished iteration 70\n",
      "Finished iteration 71\n",
      "Finished iteration 72\n",
      "Finished iteration 73\n",
      "Finished iteration 74\n",
      "Finished iteration 75\n",
      "Finished iteration 76\n",
      "Finished iteration 77\n",
      "Finished iteration 78\n",
      "Finished iteration 79\n",
      "Finished iteration 80\n",
      "Finished iteration 81\n",
      "Finished iteration 82\n",
      "Finished iteration 83\n",
      "Finished iteration 84\n",
      "Finished iteration 85\n",
      "Finished iteration 86\n",
      "Finished iteration 87\n",
      "Finished iteration 88\n",
      "Finished iteration 89\n",
      "Finished iteration 90\n",
      "Finished iteration 91\n",
      "Finished iteration 92\n",
      "Finished iteration 93\n",
      "Finished iteration 94\n",
      "Finished iteration 95\n",
      "Finished iteration 96\n",
      "Finished iteration 97\n",
      "Finished iteration 98\n",
      "Finished iteration 99\n",
      "Finished iteration 100\n",
      "Finished iteration 101\n",
      "Finished iteration 102\n",
      "Finished iteration 103\n",
      "Finished iteration 104\n",
      "Finished iteration 105\n",
      "Finished iteration 106\n",
      "Finished iteration 107\n",
      "Finished iteration 108\n",
      "Finished iteration 109\n",
      "Finished iteration 110\n",
      "Finished iteration 111\n",
      "Finished iteration 112\n",
      "Finished iteration 113\n",
      "Finished iteration 114\n",
      "Finished iteration 115\n",
      "Finished iteration 116\n",
      "Finished iteration 117\n",
      "Finished iteration 118\n",
      "Finished iteration 119\n",
      "Finished iteration 120\n",
      "Finished iteration 121\n",
      "Finished iteration 122\n",
      "Finished iteration 123\n",
      "Finished iteration 124\n",
      "Finished iteration 125\n",
      "Finished iteration 126\n",
      "Finished iteration 127\n",
      "Finished iteration 128\n",
      "Finished iteration 129\n",
      "Finished iteration 130\n",
      "Finished iteration 131\n",
      "Finished iteration 132\n",
      "Finished iteration 133\n",
      "Finished iteration 134\n",
      "Finished iteration 135\n",
      "Finished iteration 136\n",
      "Finished iteration 137\n",
      "Finished iteration 138\n",
      "Finished iteration 139\n",
      "Finished iteration 140\n",
      "Finished iteration 141\n",
      "Finished iteration 142\n",
      "Finished iteration 143\n",
      "Finished iteration 144\n",
      "Finished iteration 145\n",
      "Finished iteration 146\n",
      "Finished iteration 147\n",
      "Finished iteration 148\n",
      "Finished iteration 149\n",
      "Finished iteration 150\n",
      "Finished iteration 151\n",
      "Finished iteration 152\n",
      "Finished iteration 153\n",
      "Finished iteration 154\n",
      "Finished iteration 155\n",
      "Finished iteration 156\n",
      "Finished iteration 157\n",
      "Finished iteration 158\n",
      "Finished iteration 159\n",
      "Finished iteration 160\n",
      "Finished iteration 161\n",
      "Finished iteration 162\n",
      "Finished iteration 163\n",
      "Finished iteration 164\n",
      "Finished iteration 165\n",
      "Finished iteration 166\n",
      "Finished iteration 167\n",
      "Finished iteration 168\n",
      "Finished iteration 169\n",
      "Finished iteration 170\n",
      "Finished iteration 171\n",
      "Finished iteration 172\n",
      "Finished iteration 173\n",
      "Finished iteration 174\n",
      "Finished iteration 175\n",
      "Finished iteration 176\n",
      "Finished iteration 177\n",
      "Finished iteration 178\n",
      "Finished iteration 179\n",
      "Finished iteration 180\n",
      "Finished iteration 181\n",
      "Finished iteration 182\n",
      "Finished iteration 183\n",
      "Finished iteration 184\n",
      "Finished iteration 185\n",
      "Finished iteration 186\n",
      "Finished iteration 187\n",
      "Finished iteration 188\n",
      "Finished iteration 189\n",
      "Finished iteration 190\n",
      "Finished iteration 191\n",
      "Finished iteration 192\n",
      "Finished iteration 193\n",
      "Finished iteration 194\n",
      "Finished iteration 195\n",
      "Finished iteration 196\n",
      "Finished iteration 197\n",
      "Finished iteration 198\n",
      "Finished iteration 199\n",
      "Finished iteration 200\n",
      "Finished iteration 201\n",
      "Finished iteration 202\n",
      "Finished iteration 203\n",
      "Finished iteration 204\n",
      "Finished iteration 205\n",
      "Finished iteration 206\n",
      "Finished iteration 207\n",
      "Finished iteration 208\n",
      "Finished iteration 209\n",
      "Finished iteration 210\n",
      "Finished iteration 211\n",
      "Finished iteration 212\n",
      "Finished iteration 213\n",
      "Finished iteration 214\n",
      "Finished iteration 215\n",
      "Finished iteration 216\n",
      "Finished iteration 217\n",
      "Finished iteration 218\n",
      "Finished iteration 219\n",
      "Finished iteration 220\n",
      "Finished iteration 221\n",
      "Finished iteration 222\n",
      "Finished iteration 223\n",
      "Finished iteration 224\n",
      "Finished iteration 225\n",
      "Finished iteration 226\n",
      "Finished iteration 227\n",
      "Finished iteration 228\n",
      "Finished iteration 229\n",
      "Finished iteration 230\n",
      "Finished iteration 231\n",
      "Finished iteration 232\n",
      "Finished iteration 233\n",
      "Finished iteration 234\n",
      "Finished iteration 235\n",
      "Finished iteration 236\n",
      "Finished iteration 237\n",
      "Finished iteration 238\n",
      "Finished iteration 239\n",
      "Finished iteration 240\n",
      "Finished iteration 241\n",
      "Finished iteration 242\n",
      "Finished iteration 243\n",
      "Finished iteration 244\n",
      "Finished iteration 245\n",
      "Finished iteration 246\n",
      "Finished iteration 247\n",
      "Finished iteration 248\n",
      "Finished iteration 249\n",
      "Finished iteration 250\n",
      "Finished iteration 251\n",
      "Finished iteration 252\n",
      "Finished iteration 253\n",
      "Finished iteration 254\n",
      "Finished iteration 255\n",
      "Finished iteration 256\n",
      "Finished iteration 257\n",
      "Finished iteration 258\n",
      "Finished iteration 259\n",
      "Finished iteration 260\n",
      "Finished iteration 261\n",
      "Finished iteration 262\n",
      "Finished iteration 263\n",
      "Finished iteration 264\n",
      "Finished iteration 265\n",
      "Finished iteration 266\n",
      "Finished iteration 267\n",
      "Finished iteration 268\n",
      "Finished iteration 269\n",
      "Finished iteration 270\n",
      "Finished iteration 271\n",
      "Finished iteration 272\n",
      "Finished iteration 273\n",
      "Finished iteration 274\n",
      "Finished iteration 275\n",
      "Finished iteration 276\n",
      "Finished iteration 277\n",
      "Finished iteration 278\n",
      "Finished iteration 279\n",
      "Finished iteration 280\n",
      "Finished iteration 281\n",
      "Finished iteration 282\n",
      "Finished iteration 283\n",
      "Finished iteration 284\n",
      "Finished iteration 285\n",
      "Finished iteration 286\n",
      "Finished iteration 287\n",
      "Finished iteration 288\n",
      "Finished iteration 289\n",
      "Finished iteration 290\n",
      "Finished iteration 291\n",
      "Finished iteration 292\n",
      "Finished iteration 293\n",
      "Finished iteration 294\n",
      "Finished iteration 295\n",
      "Finished iteration 296\n",
      "Finished iteration 297\n",
      "Finished iteration 298\n",
      "Finished iteration 299\n"
     ]
    }
   ],
   "source": [
    "k_means_model = k_means(K = 10, max_iter = 300)\n",
    "k_means_predictions = k_means_model.predict(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-rebate",
   "metadata": {},
   "source": [
    "Time to evaluate the k-means results on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "early-airplane",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7f8d5b94bb42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mk_means_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_means_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mk_means_purity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_purity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_means_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_subset_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K_means with R2 purity \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_means_purity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-2700315aab82>\u001b[0m in \u001b[0;36mcalculate_purity\u001b[1;34m(predictions, number_of_classes, train_labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmost_occured\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmost_occured\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters_to_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mclusters_to_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmost_occured\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "k_means_predictions = k_means_predictions.astype(int)\n",
    "\n",
    "k_means_purity = calculate_purity(k_means_predictions, 10, train_subset_labels)\n",
    "print(\"K_means with R2 purity \\n\")\n",
    "for i in range(len(k_means_purity)):\n",
    "    print(class_names[i] + \" purity: \" + str(k_means_purity[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-collaboration",
   "metadata": {},
   "source": [
    "The error occurs because one category was not matched to a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beneficial-dinner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_means with R2 f-measure \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-9281bddad538>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K_means with R2 f-measure \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcalculate_F_measure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_means_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_subset_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-81d5f7c09f87>\u001b[0m in \u001b[0;36mcalculate_F_measure\u001b[1;34m(predictions, number_of_classes, train_labels)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mclusters_to_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mmost_occured\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmost_occured\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters_to_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mclusters_to_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmost_occured\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "print(\"K_means with R2 f-measure \\n\")\n",
    "calculate_F_measure(k_means_predictions, 10, train_subset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-valve",
   "metadata": {},
   "source": [
    "Implementing a function which computes the Kullback-Leibler distance between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "preliminary-capture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kullback_leibler_distance(P, Q):\n",
    "    P_probabilities = np.zeros((len(P)))\n",
    "    Q_probabilities = np.zeros((len(P)))\n",
    "    P_sum = np.sum(P)\n",
    "    Q_sum = np.sum(Q)\n",
    "    \n",
    "    for i in range(len(P)):\n",
    "        P_probabilities[i] = P[i] / P_sum\n",
    "        Q_probabilities[i] = Q[i] / Q_sum\n",
    "        \n",
    "    distance = 0\n",
    "    \n",
    "    for i in range(len(P_probabilities)):\n",
    "\n",
    "        if P_probabilities[i] == 0 or Q_probabilities[i] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            distance += (P_probabilities[i] - Q_probabilities[i]) * log(P_probabilities[i] /  Q_probabilities[i], 10)\n",
    "            \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-ordering",
   "metadata": {},
   "source": [
    "Trying out the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "embedded-penguin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5264454858943826\n"
     ]
    }
   ],
   "source": [
    "print(kullback_leibler_distance(R2[15], R2[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-yugoslavia",
   "metadata": {},
   "source": [
    "Bellow I implement the k_medoids algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "another-affiliate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# max_iter is the maximum iterations that we allow in case we don't converge earlier\n",
    "# k = 10 for k clusters (since the clothing categories are actually 10)\n",
    "\n",
    "class k_medoids:\n",
    "    def __init__(self, K = 10, max_iter = 300, tolerance = 0.001):\n",
    "        self.K = K\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        # list of sample indices for each cluster\n",
    "        self.clusters = [[] for i in range(self.K)]\n",
    "        \n",
    "        # medoid point for each cluster\n",
    "        self.medoids = []\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        print(X.shape)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        print(\"Starting algorithm\")\n",
    "        \n",
    "        # initialise medoids (10 of the histogram representations will be picked randomly)\n",
    "        random_sample_indices = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.medoids = [self.X[index] for index in random_sample_indices]\n",
    "        \n",
    "        # optimisation\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # update clusters\n",
    "            self.clusters = self._create_clusters(self.medoids)\n",
    "            \n",
    "            # update centroids\n",
    "            medoids_old = self.medoids # in order to check for convergence\n",
    "            self.medoids, change = self._get_medoids(self.clusters)\n",
    "            \n",
    "            # check if medoids changed\n",
    "            if not change:\n",
    "                break\n",
    "            \n",
    "            print(\"Finished iteration\", i)\n",
    "            \n",
    "        # return cluster labels\n",
    "        return self.__get_cluster_labels(self.clusters)\n",
    "    \n",
    "    def __get_cluster_labels(self, clusters):\n",
    "        labels = np.empty(self.n_samples)\n",
    "        for cluster_index, cluster in enumerate(clusters):\n",
    "            for sample_index in cluster:\n",
    "                labels[sample_index] = cluster_index\n",
    "                \n",
    "        return labels\n",
    "       \n",
    "    # assign each sample to the cluster of the closest medoid\n",
    "    def _create_clusters(self, medoids):\n",
    "        clusters = [[] for i in range(self.K)]\n",
    "        \n",
    "        for index, sample in enumerate(self.X):\n",
    "            medoid_index = self._closest_medoid(sample, medoids)\n",
    "            clusters[medoid_index].append(index)\n",
    "            \n",
    "        return clusters\n",
    "    \n",
    "    # calculate the distances of the current sample from each medoid (according to Kullback-Leibler distance)\n",
    "    def _closest_medoid(self, sample, medoids):\n",
    "        distances = [kullback_leibler_distance(sample, medoid) for medoid in medoids]\n",
    "        closest_index = np.argmin(distances)\n",
    "        return closest_index\n",
    "    \n",
    "    # calculate the centroids as the mean value of the samples in each cluster\n",
    "    def _get_medoids(self, clusters):\n",
    "        medoid_change = False\n",
    "        medoids_new = np.zeros((self.K, self.n_features))\n",
    "        old_distance_sum = self._calculate_distances(self.medoids, self.clusters)\n",
    "        \n",
    "        for cluster_index, cluster in enumerate(clusters):\n",
    "            previous_dist = self._calculate_cluster_distance(self.medoids[cluster_index], cluster)\n",
    "            best_sample_index = 0\n",
    "            lesser_distance = 0 \n",
    "            \n",
    "            for sample_index, sample in enumerate(cluster):\n",
    "                \n",
    "                if (self.X[sample] != self.medoids[cluster_index]).all():\n",
    "                    distance = self._calculate_cluster_distance(self.X[sample], cluster)\n",
    "                    if distance < lesser_distance:\n",
    "                        lesser_distance = distance\n",
    "                        best_sample_index = sample_index\n",
    "                        \n",
    "            if lesser_distance < previous_dist:\n",
    "                medoid_change = True\n",
    "                medoids_new[cluster_index] = self.X[cluster[best_sample_index]]\n",
    "            else:\n",
    "                medoids_new[cluster_index] = self.medoids[cluster_index]\n",
    "                \n",
    "       \n",
    "        return medoids_new, medoid_change\n",
    "    \n",
    "    # check for convergence\n",
    "    def _is_converged(self, centroids_old, centroids):\n",
    "        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n",
    "        return sum(distances) <= self.tolerance\n",
    "    \n",
    "    # calculate the sum of all distances of the samples from their cluster's medoid\n",
    "    def _calculate_distances(self, medoids, clusters):\n",
    "        distance = 0\n",
    "        \n",
    "        for cluster_index, cluster in enumerate(clusters):\n",
    "            for sample in cluster:\n",
    "                distance += kullback_leibler_distance(self.X[sample], medoids[cluster_index])\n",
    "                \n",
    "        return distance\n",
    "        \n",
    "    # calculate the sum of all distances of a sample in each cluster  \n",
    "    def _calculate_cluster_distance(self, sample, cluster):\n",
    "        distance = 0\n",
    "        \n",
    "        for item in cluster:\n",
    "            distance += kullback_leibler_distance(sample, self.X[item])\n",
    "                \n",
    "        return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-reason",
   "metadata": {},
   "source": [
    "Predictions using the k-medoids algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "worthy-badge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32)\n",
      "Starting algorithm\n",
      "Finished iteration 0\n",
      "Finished iteration 1\n",
      "Finished iteration 2\n",
      "Finished iteration 3\n",
      "Finished iteration 4\n",
      "Finished iteration 5\n",
      "Finished iteration 6\n",
      "Finished iteration 7\n",
      "Finished iteration 8\n",
      "Finished iteration 9\n",
      "Finished iteration 10\n",
      "Finished iteration 11\n",
      "Finished iteration 12\n",
      "Finished iteration 13\n",
      "Finished iteration 14\n",
      "Finished iteration 15\n",
      "Finished iteration 16\n",
      "Finished iteration 17\n",
      "Finished iteration 18\n",
      "Finished iteration 19\n"
     ]
    }
   ],
   "source": [
    "k_medoids_model = k_medoids(K = 10, max_iter = 20, tolerance=0.001)\n",
    "k_medoids_predictions = k_medoids_model.predict(R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daily-bubble",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 1. 0. 7. 6. 3. 4. 7. 3. 2. 6. 6. 0. 3. 6. 6. 5. 0. 6. 3. 1. 2. 1. 8.\n",
      " 4. 1. 7. 0. 0. 9. 0. 8. 1. 0. 7. 3. 0. 0. 6. 1. 3. 6. 0. 6. 4. 1. 3. 3.\n",
      " 3. 3. 1. 9. 2. 4. 0. 1. 7. 0. 6. 7. 0. 3. 1. 0. 2. 3. 9. 8. 3. 9. 8. 1.\n",
      " 0. 3. 3. 9. 8. 0. 0. 0. 3. 8. 7. 0. 0. 6. 2. 3. 6. 3. 4. 2. 4. 1. 6. 6.\n",
      " 4. 8. 7. 8. 6. 6. 0. 2. 6. 6. 2. 0. 0. 2. 6. 0. 0. 8. 3. 0. 3. 0. 0. 0.\n",
      " 2. 4. 0. 0. 3. 0. 0. 0. 2. 6. 0. 3. 0. 2. 2. 0. 2. 2. 3. 7. 0. 2. 3. 2.\n",
      " 0. 2. 2. 0. 3. 2. 3. 3. 3. 0. 3. 0. 0. 1. 0. 6. 0. 7. 0. 0. 0. 6. 6. 2.\n",
      " 0. 0. 3. 2. 2. 0. 3. 2. 0. 0. 0. 0. 1. 3. 3. 6. 5. 6. 2. 1. 0. 4. 0. 0.\n",
      " 7. 5. 3. 0. 2. 2. 7. 2. 6. 9. 5. 1. 3. 9. 4. 7. 4. 4. 0. 0. 0. 9. 5. 1.\n",
      " 0. 7. 6. 5. 3. 6. 7. 3. 3. 8. 0. 0. 5. 9. 3. 9. 4. 0. 4. 4. 4. 3. 9. 7.\n",
      " 4. 3. 4. 1. 3. 3. 9. 3. 3. 0. 8. 7. 9. 0. 3. 8. 5. 0. 8. 5. 7. 9. 7. 6.\n",
      " 6. 7. 9. 6. 7. 6. 4. 5. 1. 0. 9. 4. 5. 1. 9. 6. 8. 3. 7. 8. 9. 1. 0. 8.\n",
      " 9. 4. 4. 3. 0. 6. 9. 6. 4. 9. 9. 8. 3. 8. 0. 3. 0. 0. 0. 6. 9. 6. 0. 0.\n",
      " 0. 3. 6. 3. 9. 3. 7. 3. 6. 2. 0. 2. 4. 0. 3. 9. 1. 0. 8. 1. 4. 6. 6. 2.\n",
      " 0. 0. 0. 0. 6. 8. 0. 0. 7. 6. 4. 0. 0. 9. 9. 6. 3. 3. 2. 7. 7. 0. 8. 3.\n",
      " 0. 3. 9. 1. 6. 2. 2. 7. 2. 3. 2. 2. 4. 7. 3. 1. 1. 0. 0. 0. 0. 6. 5. 0.\n",
      " 2. 2. 2. 2. 0. 6. 9. 4. 0. 0. 0. 8. 1. 0. 9. 8. 4. 3. 6. 3. 6. 3. 0. 0.\n",
      " 4. 3. 9. 1. 8. 8. 9. 3. 6. 3. 3. 3. 3. 7. 0. 6. 7. 0. 5. 6. 4. 9. 8. 6.\n",
      " 6. 1. 6. 6. 0. 3. 4. 0. 9. 3. 8. 6. 3. 6. 0. 4. 3. 2. 6. 5. 2. 0. 6. 6.\n",
      " 7. 3. 8. 8. 9. 9. 0. 8. 6. 8. 3. 0. 6. 3. 3. 4. 7. 0. 7. 0. 6. 3. 4. 3.\n",
      " 0. 0. 3. 0. 3. 6. 5. 3. 7. 0. 6. 0. 6. 6. 0. 8. 7. 9. 7. 6. 0. 0. 1. 3.\n",
      " 0. 3. 2. 2. 0. 1. 4. 2. 0. 0. 9. 2. 7. 3. 3. 0. 2. 6. 1. 3. 6. 0. 3. 8.\n",
      " 1. 3. 0. 0. 1. 2. 0. 7. 1. 0. 3. 4. 3. 5. 0. 3. 3. 0. 0. 3. 2. 1. 2. 0.\n",
      " 0. 2. 0. 0. 2. 2. 3. 0. 2. 2. 0. 0. 0. 0. 0. 9. 0. 7. 2. 5. 3. 2. 3. 0.\n",
      " 0. 1. 0. 0. 1. 0. 9. 0. 1. 3. 3. 4. 0. 7. 0. 3. 1. 2. 1. 0. 2. 5. 2. 3.\n",
      " 7. 3. 4. 0. 6. 6. 0. 3. 7. 3. 3. 3. 3. 4. 3. 3. 6. 3. 6. 1. 3. 7. 3. 0.\n",
      " 8. 3. 9. 3. 1. 4. 4. 4. 6. 0. 6. 3. 3. 3. 3. 1. 3. 3. 4. 7. 9. 1. 6. 3.\n",
      " 1. 3. 4. 0. 9. 9. 3. 0. 6. 3. 3. 6. 4. 3. 2. 7. 3. 4. 4. 6. 0. 3. 6. 4.\n",
      " 9. 0. 1. 3. 4. 3. 4. 3. 4. 1. 2. 4. 3. 7. 3. 7. 5. 3. 3. 7. 0. 1. 7. 0.\n",
      " 6. 4. 6. 7. 0. 1. 0. 2. 2. 6. 3. 2. 2. 0. 2. 0. 6. 2. 6. 4. 0. 6. 2. 2.\n",
      " 2. 2. 0. 6. 6. 1. 0. 0. 3. 5. 0. 0. 0. 1. 6. 6. 7. 3. 0. 2. 2. 2. 2. 0.\n",
      " 2. 2. 3. 2. 2. 0. 2. 3. 8. 4. 6. 2. 0. 0. 3. 0. 2. 2. 0. 6. 8. 2. 1. 0.\n",
      " 0. 0. 0. 6. 0. 4. 4. 2. 1. 3. 7. 1. 3. 0. 0. 0. 3. 2. 2. 2. 0. 1. 0. 1.\n",
      " 0. 3. 0. 3. 0. 6. 0. 3. 3. 3. 2. 6. 8. 8. 9. 0. 7. 5. 3. 3. 6. 3. 6. 0.\n",
      " 0. 3. 0. 8. 0. 7. 1. 0. 6. 8. 8. 6. 3. 0. 7. 0. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 1. 3. 6. 0. 0. 8. 0. 3. 0. 2. 0. 0. 8. 8. 6. 8. 1. 2. 0. 6. 6. 4. 6. 0.\n",
      " 7. 8. 1. 8. 0. 7. 2. 0. 1. 0. 0. 3. 8. 6. 0. 7. 3. 8. 6. 0. 3. 2. 8. 2.\n",
      " 0. 3. 0. 0. 1. 6. 6. 0. 6. 0. 8. 6. 8. 2. 5. 9. 9. 0. 0. 0. 6. 3. 8. 0.\n",
      " 0. 0. 6. 6. 6. 6. 3. 7. 2. 3. 2. 0. 2. 0. 9. 0. 0. 6. 3. 6. 0. 3. 0. 0.\n",
      " 9. 0. 0. 4. 0. 3. 6. 3. 7. 6. 6. 7. 2. 9. 6. 0. 3. 6. 3. 2. 3. 0. 6. 8.\n",
      " 3. 3. 2. 3. 7. 6. 6. 9. 2. 7. 0. 8. 6. 6. 8. 0. 6. 8. 6. 3. 0. 0. 0. 1.\n",
      " 0. 0. 8. 9. 3. 6. 4. 2. 6. 6. 0. 6. 2. 6. 6. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(k_medoids_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "substantial-skating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1 0 7 6 3 4 7 3 2 6 6 0 3 6 6 5 0 6 3 1 2 1 8 4 1 7 0 0 9 0 8 1 0 7 3 0\n",
      " 0 6 1 3 6 0 6 4 1 3 3 3 3 1 9 2 4 0 1 7 0 6 7 0 3 1 0 2 3 9 8 3 9 8 1 0 3\n",
      " 3 9 8 0 0 0 3 8 7 0 0 6 2 3 6 3 4 2 4 1 6 6 4 8 7 8 6 6 0 2 6 6 2 0 0 2 6\n",
      " 0 0 8 3 0 3 0 0 0 2 4 0 0 3 0 0 0 2 6 0 3 0 2 2 0 2 2 3 7 0 2 3 2 0 2 2 0\n",
      " 3 2 3 3 3 0 3 0 0 1 0 6 0 7 0 0 0 6 6 2 0 0 3 2 2 0 3 2 0 0 0 0 1 3 3 6 5\n",
      " 6 2 1 0 4 0 0 7 5 3 0 2 2 7 2 6 9 5 1 3 9 4 7 4 4 0 0 0 9 5 1 0 7 6 5 3 6\n",
      " 7 3 3 8 0 0 5 9 3 9 4 0 4 4 4 3 9 7 4 3 4 1 3 3 9 3 3 0 8 7 9 0 3 8 5 0 8\n",
      " 5 7 9 7 6 6 7 9 6 7 6 4 5 1 0 9 4 5 1 9 6 8 3 7 8 9 1 0 8 9 4 4 3 0 6 9 6\n",
      " 4 9 9 8 3 8 0 3 0 0 0 6 9 6 0 0 0 3 6 3 9 3 7 3 6 2 0 2 4 0 3 9 1 0 8 1 4\n",
      " 6 6 2 0 0 0 0 6 8 0 0 7 6 4 0 0 9 9 6 3 3 2 7 7 0 8 3 0 3 9 1 6 2 2 7 2 3\n",
      " 2 2 4 7 3 1 1 0 0 0 0 6 5 0 2 2 2 2 0 6 9 4 0 0 0 8 1 0 9 8 4 3 6 3 6 3 0\n",
      " 0 4 3 9 1 8 8 9 3 6 3 3 3 3 7 0 6 7 0 5 6 4 9 8 6 6 1 6 6 0 3 4 0 9 3 8 6\n",
      " 3 6 0 4 3 2 6 5 2 0 6 6 7 3 8 8 9 9 0 8 6 8 3 0 6 3 3 4 7 0 7 0 6 3 4 3 0\n",
      " 0 3 0 3 6 5 3 7 0 6 0 6 6 0 8 7 9 7 6 0 0 1 3 0 3 2 2 0 1 4 2 0 0 9 2 7 3\n",
      " 3 0 2 6 1 3 6 0 3 8 1 3 0 0 1 2 0 7 1 0 3 4 3 5 0 3 3 0 0 3 2 1 2 0 0 2 0\n",
      " 0 2 2 3 0 2 2 0 0 0 0 0 9 0 7 2 5 3 2 3 0 0 1 0 0 1 0 9 0 1 3 3 4 0 7 0 3\n",
      " 1 2 1 0 2 5 2 3 7 3 4 0 6 6 0 3 7 3 3 3 3 4 3 3 6 3 6 1 3 7 3 0 8 3 9 3 1\n",
      " 4 4 4 6 0 6 3 3 3 3 1 3 3 4 7 9 1 6 3 1 3 4 0 9 9 3 0 6 3 3 6 4 3 2 7 3 4\n",
      " 4 6 0 3 6 4 9 0 1 3 4 3 4 3 4 1 2 4 3 7 3 7 5 3 3 7 0 1 7 0 6 4 6 7 0 1 0\n",
      " 2 2 6 3 2 2 0 2 0 6 2 6 4 0 6 2 2 2 2 0 6 6 1 0 0 3 5 0 0 0 1 6 6 7 3 0 2\n",
      " 2 2 2 0 2 2 3 2 2 0 2 3 8 4 6 2 0 0 3 0 2 2 0 6 8 2 1 0 0 0 0 6 0 4 4 2 1\n",
      " 3 7 1 3 0 0 0 3 2 2 2 0 1 0 1 0 3 0 3 0 6 0 3 3 3 2 6 8 8 9 0 7 5 3 3 6 3\n",
      " 6 0 0 3 0 8 0 7 1 0 6 8 8 6 3 0 7 0 3 3 3 3 3 3 3 3 1 3 6 0 0 8 0 3 0 2 0\n",
      " 0 8 8 6 8 1 2 0 6 6 4 6 0 7 8 1 8 0 7 2 0 1 0 0 3 8 6 0 7 3 8 6 0 3 2 8 2\n",
      " 0 3 0 0 1 6 6 0 6 0 8 6 8 2 5 9 9 0 0 0 6 3 8 0 0 0 6 6 6 6 3 7 2 3 2 0 2\n",
      " 0 9 0 0 6 3 6 0 3 0 0 9 0 0 4 0 3 6 3 7 6 6 7 2 9 6 0 3 6 3 2 3 0 6 8 3 3\n",
      " 2 3 7 6 6 9 2 7 0 8 6 6 8 0 6 8 6 3 0 0 0 1 0 0 8 9 3 6 4 2 6 6 0 6 2 6 6\n",
      " 6]\n"
     ]
    }
   ],
   "source": [
    "k_medoids_predictions = k_medoids_predictions.astype(int)\n",
    "print(k_medoids_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-dragon",
   "metadata": {},
   "source": [
    "Evaluating the k-medoids result using the functions I implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "separate-buffer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 7, 6, 2, 4, 9, 3, 8, 5]\n",
      "K_medoids purity \n",
      "\n",
      "T-shirt/top purity: 0.12\n",
      "Trouser purity: 0.39\n",
      "Pullover purity: 0.14\n",
      "Dress purity: 0.06\n",
      "Coat purity: 0.03\n",
      "Sandal purity: 0.03\n",
      "Shirt purity: 0.34\n",
      "Sneaker purity: 0.27\n",
      "Bag purity: 0.15\n",
      "Ankle boot purity: 0.26\n"
     ]
    }
   ],
   "source": [
    "k_medoids_purity = calculate_purity(k_medoids_predictions, 10, train_subset_labels)\n",
    "print(\"K_medoids purity \\n\")\n",
    "for i in range(len(k_medoids_purity)):\n",
    "    print(class_names[i] + \" purity: \" + str(k_medoids_purity[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "frozen-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.179\n"
     ]
    }
   ],
   "source": [
    "purity = np.mean(k_medoids_purity)\n",
    "print(purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "innocent-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_medoids f-measure \n",
      "\n",
      "[1, 0, 7, 6, 2, 4, 9, 3, 8, 5]\n",
      "Cluster 0  f-measure: 0.22285714285714286\n",
      "Cluster 1  f-measure: 0.15894039735099338\n",
      "Cluster 2  f-measure: 0.2621359223300971\n",
      "Cluster 3  f-measure: 0.26877470355731226\n",
      "Cluster 4  f-measure: 0.2545454545454546\n",
      "Cluster 5  f-measure: 0.060000000000000005\n",
      "Cluster 6  f-measure: 0.22510822510822512\n",
      "Cluster 7  f-measure: 0.10084033613445377\n",
      "Cluster 8  f-measure: 0.189873417721519\n",
      "Cluster 9  f-measure: 0.05769230769230769\n",
      "Method total f-measure: 1.8007679072975058\n"
     ]
    }
   ],
   "source": [
    "print(\"K_medoids f-measure \\n\")\n",
    "calculate_F_measure(k_medoids_predictions, 10, train_subset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-metallic",
   "metadata": {},
   "source": [
    "Bellow I implement a class for the hierarchical divisive clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "solid-craps",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class hierarchical_divisive_clustering:\n",
    "    def __init__(self, K = 10):\n",
    "        self.K = K\n",
    "        \n",
    "        # in hierarchical divisive clustering we start with only one cluster for the whole set\n",
    "        self.clusters = []\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.clusters.append([index for index in range(self.n_samples)])\n",
    "       \n",
    "        divCounter = 1\n",
    "        while len(self.clusters) < self.K: # keep dividing\n",
    "            print(\"Division \", divCounter)\n",
    "            \n",
    "            # calculate max variance of each cluster in order to decide which to split\n",
    "            cluster_to_split_index = self.max_variance_criterion(self.clusters)\n",
    "            \n",
    "            # remove the cluster to be splitted from the list, split it and add the two new clusters to the list\n",
    "            cluster_to_split = self.clusters.pop(cluster_to_split_index)\n",
    "            newCluster1, newCluster2 = self.split_cluster(cluster_to_split)\n",
    "            self.clusters.append(newCluster1)\n",
    "            self.clusters.append(newCluster2)\n",
    "            \n",
    "            divCounter += 1\n",
    "            \n",
    "        labels = self.__get_cluster_labels(self.clusters)\n",
    "        return labels\n",
    "            \n",
    "    def max_variance_criterion(self, clusters):\n",
    "        variances = np.zeros(len(clusters))\n",
    "\n",
    "        for i in range(len(clusters)):\n",
    "            variances[i] = self.calculate_cluster_variance(clusters[i])\n",
    "\n",
    "        index = np.argmax(variances)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def calculate_cluster_variance(self, cluster):\n",
    "        mean_value = np.zeros(self.n_features)\n",
    "\n",
    "        for i in range(len(cluster)):\n",
    "            mean_value = np.add(mean_value, self.X[cluster[i]])\n",
    "\n",
    "        if(len(cluster) > 0):\n",
    "            mean_value = mean_value / len(cluster)\n",
    "\n",
    "        all_features = []\n",
    "\n",
    "        for i in range(len(cluster)):\n",
    "            all_features.append(self.X[cluster[i]])\n",
    "\n",
    "        deviations = [np.subtract(x, mean_value) ** 2 for x in all_features]\n",
    "\n",
    "        variance = np.zeros(self.n_features)\n",
    "\n",
    "        for i in range(len(deviations)):\n",
    "            variance = np.add(variance, deviations[i])\n",
    "\n",
    "        if(len(cluster) > 0):\n",
    "            return np.sum(variance / len(cluster))\n",
    "        \n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "    def __get_cluster_labels(self, clusters):\n",
    "        labels = np.empty(self.n_samples)\n",
    "        for cluster_index, cluster in enumerate(clusters):\n",
    "            for sample_index in cluster:\n",
    "                labels[sample_index] = cluster_index\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def split_cluster(self, cluster):\n",
    "        distances = np.zeros((self.n_samples, self.n_samples))\n",
    "\n",
    "        for i in range(len(cluster)):\n",
    "            for j in range(len(cluster)):\n",
    "                for n in range(len(cluster)):\n",
    "                    dist1 = euclidean_distance(self.X[cluster[n]], self.X[cluster[i]])\n",
    "                    dist2 = euclidean_distance(self.X[cluster[n]], self.X[cluster[j]])\n",
    "                    if dist1 <= dist2:\n",
    "                        distances[i][j] = np.add(distances[i][j], dist1)\n",
    "                    else:\n",
    "                        distances[i][j] = np.add(distances[i][j], dist2)\n",
    "\n",
    "        (x1, x2) = np.unravel_index(distances.argmin(), distances.shape)       \n",
    "        cluster1 = []\n",
    "        cluster2 = []\n",
    "\n",
    "        for i in range(len(cluster)):\n",
    "            if (euclidean_distance(self.X[cluster[i]], self.X[x1]) <= euclidean_distance(self.X[cluster[i]], self.X[x2])):\n",
    "                cluster1.append(cluster[i])\n",
    "            else:\n",
    "                cluster2.append(cluster[i])\n",
    "\n",
    "        return cluster1, cluster2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-teddy",
   "metadata": {},
   "source": [
    "Here I will reduce the size of the dataset even further (200 samples), because the algorithm I implemented was slow because of the triple for-loop I implement for the choice of samples on the base of which the cluster will be divided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "danish-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_size = 200\n",
    "train_subset_images = []\n",
    "train_subset_labels = []\n",
    "\n",
    "for category_index in range(10):\n",
    "    remaining_from_category = train_subset_size / 10 # we need equally distributed data per category\n",
    "    image_index = 0\n",
    "    \n",
    "    while remaining_from_category > 0 :\n",
    "        if train_labels[image_index] == category_index:\n",
    "            train_subset_images.append(train_images[image_index])\n",
    "            train_subset_labels.append(train_labels[image_index])\n",
    "            remaining_from_category -= 1\n",
    "        image_index += 1\n",
    "        \n",
    "# R1 dataset : 28x28 = 784 feature vector\n",
    "\n",
    "train_subset_images = np.array(train_subset_images)\n",
    "train_subset_labels = np.array(train_subset_labels)\n",
    "\n",
    "nsamples, nx, ny = train_subset_images.shape \n",
    "train_subset_images_R1 = train_subset_images.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eastern-coaching",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division  1\n",
      "Division  2\n",
      "Division  3\n",
      "Division  4\n",
      "Division  5\n",
      "Division  6\n",
      "Division  7\n",
      "Division  8\n",
      "Division  9\n"
     ]
    }
   ],
   "source": [
    "hierarchical_divisive_clustering_model = hierarchical_divisive_clustering(K = 10)\n",
    "hierarchical_divisive_clustering_predictions = hierarchical_divisive_clustering_model.predict(train_subset_images_R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "awful-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 3 2 1 1 8 3 1 3 3 1 3 3 1 8 3 0 8 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 2 8 8 8 3 0 0 3 8 3 3 8 0 8 0 0 3 8 8 8 0 3 2 3 3 2 2 3 3 3 3 0 2 3 3\n",
      " 3 3 3 8 3 3 3 3 0 0 0 0 3 3 3 0 0 3 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 8 0 3 0 0 8 3 3 8 0 3 0 3 3 3 0 8 3 8 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 8 3 3 0 0 0 0 0 3 8 3 3 0 3 0 0 3 3 3 8 0 0 3 3 0\n",
      " 3 8 8 3 3 3 3 3 0 3 3 8 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "hierarchical_divisive_clustering_predictions = hierarchical_divisive_clustering_predictions.astype(int)\n",
    "print(hierarchical_divisive_clustering_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-nature",
   "metadata": {},
   "source": [
    "Again, there are clothing categories to which not even one datapoint was matched to, so an error occurs on the stats calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "twenty-chain",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-cad043bd245b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcalculate_purity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhierarchical_divisive_clustering_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_subset_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-2700315aab82>\u001b[0m in \u001b[0;36mcalculate_purity\u001b[1;34m(predictions, number_of_classes, train_labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmost_occured\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmost_occured\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters_to_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mclusters_to_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmost_occured\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "calculate_purity(hierarchical_divisive_clustering_predictions, 10, train_subset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-colon",
   "metadata": {},
   "source": [
    "In total, the k-means algorithm on the first dataset achieved a purity score of 0.25 and an f-score of 3.05, whereas the k-medoids algorithm (R2-set) achieved a purity score of 0.179 and an f-score of 1.8. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
